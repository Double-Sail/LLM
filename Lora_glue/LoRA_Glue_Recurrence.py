import sys
from datasets import load_dataset, load_metric, concatenate_datasets
import datasets
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForMaskedLM, \
    DataCollatorWithPadding
import numpy as np
from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, \
    TrainingArguments, Trainer, DataCollatorForSeq2Seq
import os
import evaluate

# dataset_name : mrpcã€colaã€qnliã€qqpã€rteã€stsbã€mnliã€sst2ã€
dataset_name = 'mnli'
# model_name : flan-t5-xxl,flan-t5-base,flan-t5-large,roberta-large,bert-large-uncased,bert-large-cased,gpt-2,gpt-3
model_name = 'roberta-large'
# glue
dataset_series_name = 'glue'

output_dir = f"./output/yg1997/{model_name}/{dataset_name}".replace('/', os.sep)
data_dir = f'./data/yg1997/{dataset_series_name}/{dataset_name}'.replace('/', os.sep)
model_dir = f'./model/{model_name}'.replace('/', os.sep)

learning_rate = 1e-4
epochs = 20

# è®°å½•ï¼š
# å·²å®Œæˆï¼šmrpc,cola,rte,sst2,stsb,qnli,mnli,qqp
# å¾…å®Œæˆï¼š(äºŒåˆ†ç±»)mrpc,cola,rte,sst2,qqp,qnli
#       (å¤šåˆ†ç±»)mnli(3)
#       (å›å½’ä»»åŠ¡)stsb
num_lables = 1 if dataset_name in ['stsb'] else (3 if dataset_name in ['mnli'] else 2)


# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# n_gpu = torch.cuda.device_count()
# print("ä½¿ç”¨çš„ GPU æ•°é‡:", n_gpu)

def get_model_tokenizer():
    print(f'å¼€å§‹å¯¼å…¥{model_name}æ¨¡å‹')
    model = None
    if model_name == 'roberta-large':
        tokenizer = AutoTokenizer.from_pretrained(model_name, resume_download=resume_download_config)
        if dataset_name in ['mrpc', 'cola', 'rte', 'sst2', 'qqp', 'qnli', 'stsb', 'mnli']:
            model = AutoModelForSequenceClassification.from_pretrained(model_name,
                                                                       cache_dir=model_dir,
                                                                       # load_in_8bit=True,
                                                                       device_map={
                                                                           "": int(os.environ.get("LOCAL_RANK") or 0)},
                                                                       # device_map="auto",
                                                                       resume_download=resume_download_config,
                                                                       num_labels=num_lables)
    else:
        print("é”™è¯¯çš„æ¨¡å‹åç§°ï¼Œè·å–æ¨¡å‹å¤±è´¥ï¼Œé€€å‡ºï¼")
        sys.exit(0)

    # è¿›è¡Œæ•´æ•°é‡åŒ–è®­ç»ƒï¼Œä»¥è·å¾—æ•´æ•°é‡åŒ–åçš„æ¨¡å‹ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„æ¨ç†æ•ˆç‡
    # å³è¿›è¡Œæ•´æ•°é‡åŒ–è®­ç»ƒï¼Œä»¥è·å¾—æ•´æ•°é‡åŒ–åçš„æ¨¡å‹ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„æ¨ç†æ•ˆç‡
    # INT8è®­ç»ƒé€šè¿‡å°†æƒé‡å’Œæ¿€æ´»å€¼å‹ç¼©ä¸º8ä½æ•´æ•°ï¼Œè€Œä¸æ˜¯æµ®ç‚¹æ•°(æ¨¡å‹é€šå¸¸ä½¿ç”¨æµ®ç‚¹ç²¾åº¦ï¼ˆå¦‚32ä½æˆ–16ä½ï¼‰æ¥è¡¨ç¤ºæƒé‡å’Œæ¿€æ´»)
    # æ¥æä¾›æ›´é«˜çš„è®¡ç®—æ•ˆç‡å’Œå†…å­˜åˆ©ç”¨ç‡ã€‚
    # model = prepare_model_for_int8_training(model)
    # é…ç½®loraå‚æ•°
    print('å¼€å§‹é…ç½®loraå‚æ•°')
    lora_config = get_lora_config(dataset_name)
    # å°†loraå’Œbaseæ¨¡å‹ç»“åˆ
    print('å¼€å§‹å°†loraå’Œbaseç»“åˆ')
    model = get_peft_model(model, lora_config)
    print('æ¨¡å‹å¤„ç†å®Œæˆ')
    return model, tokenizer


def get_lora_config(dataset_name):
    """
    æ ¹æ®ä»»åŠ¡çš„ä¸åŒï¼Œå¾—åˆ°ä¸åŒçš„lora_config
    SEQ_CLS="SEQ_CLS" ï¼ˆåºåˆ—åˆ†ç±»ï¼ŒSequence Classificationï¼‰ï¼š
                      è¿™ç§ä»»åŠ¡ç±»å‹è¦æ±‚å°†è¾“å…¥åºåˆ—åˆ†ç±»ä¸ºé¢„å®šä¹‰çš„å‡ ä¸ªç±»åˆ«ä¹‹ä¸€ã€‚è¿™ç§ä»»åŠ¡é€šå¸¸ç”¨äºæ–‡æœ¬åˆ†ç±»ã€
                      æƒ…æ„Ÿåˆ†æç­‰åœºæ™¯ã€‚æ¨¡å‹çš„è¾“å‡ºæ˜¯ä¸€ä¸ªç±»åˆ«æ ‡ç­¾ã€‚
    SEQ_2_SEQ_LM = "SEQ_2_SEQ_LM" ï¼ˆåºåˆ—åˆ°åºåˆ—çš„è¯­è¨€å»ºæ¨¡ï¼ŒSequence-to-Sequence Language Modelingï¼‰ï¼š
                                   è¿™ç§ä»»åŠ¡ç±»å‹è¦æ±‚å°†è¾“å…¥åºåˆ—ç¿»è¯‘æˆ–è½¬æ¢ä¸ºå¦ä¸€ä¸ªåºåˆ—ã€‚è¿™ç§ä»»åŠ¡é€šå¸¸ç”¨äºæœºå™¨ç¿»è¯‘ã€
                                   æ–‡æœ¬ç”Ÿæˆç­‰åœºæ™¯ã€‚æ¨¡å‹çš„è¾“å‡ºæ˜¯ç”Ÿæˆçš„åºåˆ—ã€‚
    CAUSAL_LM = "CAUSAL_LM" ï¼ˆå› æœè¯­è¨€å»ºæ¨¡ï¼ŒCausal Language Modelingï¼‰ï¼š
                              è¿™ç§ä»»åŠ¡ç±»å‹è¦æ±‚æ ¹æ®è¾“å…¥çš„éƒ¨åˆ†åºåˆ—é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯æˆ–è¯è¯­ã€‚
                              è¿™ç§ä»»åŠ¡é€šå¸¸ç”¨äºè‡ªåŠ¨è¡¥å…¨ã€è¯­è¨€ç”Ÿæˆç­‰åœºæ™¯ã€‚æ¨¡å‹çš„è¾“å‡ºæ˜¯ä¸‹ä¸€ä¸ªé¢„æµ‹çš„å•è¯æˆ–è¯è¯­ã€‚
    TOKEN_CLS = "TOKEN_CLS" ï¼ˆä»¤ç‰Œåˆ†ç±»ï¼ŒToken Classificationï¼‰ï¼š
                             è¿™ç§ä»»åŠ¡ç±»å‹è¦æ±‚å°†è¿™ä¸€åˆ†åºåˆ—ä¸­çš„æ¶ˆæ¯ä»¤ç‰Œæ ‡è®°ä¸ºé¢„å®šä½ç½®çš„å‡ ä¸­ç±»åˆ«ä»¥ä¸Šã€‚
                             è¿™ç§ä»»åŠ¡ç±»å‹è¦æ±‚å°†è¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸ªä»¤ç‰Œæ ‡è®°ä¸ºé¢„å®šä¹‰çš„å‡ ä¸ªç±»åˆ«ä¹‹ä¸€ã€‚
                             è¿™ç§ä»»åŠ¡é€šå¸¸ç”¨äºå‘½åå®ä½“è¯†åˆ«ã€è¯æ€§æ ‡æ³¨ç­‰åœºæ™¯ã€‚
                             æ¨¡å‹çš„è¾“å‡ºæ˜¯å¯¹è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªä»¤ç‰Œè¿›è¡Œçš„åˆ†ç±»æ ‡ç­¾ã€‚
    """
    if dataset_name in ['mrpc', 'cola', 'rte', 'sst2', 'qqp', 'qnli', 'stsb', 'mnli']:
        lora_config = LoraConfig(
            r=16,
            lora_alpha=32,
            target_modules=["query", "value"],
            # å³è®ºæ–‡å›¾ä¸­å³è¾¹ä¸¤ä¸ªæ¨¡å—è¾“å‡ºçš„dropout
            lora_dropout=0.05,
            bias="none",
            # è¯´æ˜ä»»åŠ¡ç±»å‹ï¼Œä»è€Œå½±å“æ¨¡å‹çš„æ¶æ„ï¼Œlossï¼Œè¾“å‡ºå½¢å¼
            task_type=TaskType.SEQ_CLS
        )
    else:
        print("é”™è¯¯çš„æ•°æ®é›†åç§°ï¼Œlora_configç”Ÿæˆå¤±è´¥ï¼Œé€€å‡ºï¼")
        sys.exit(0)

    return lora_config


def get_dataset():
    if dataset_name not in ['mnli', 'sst2', 'mrpc', 'cola', 'qnli', 'qqp', 'rte', 'stsb']:
        print("é”™è¯¯æ•°æ®é›†åç§°,ä¸‹è½½æ•°æ®é›†å¤±è´¥ï¼Œé€€å‡ºï¼")
        sys.exit(0)
    print(f'å¼€å§‹ä¸‹è½½{dataset_name}æ•°æ®é›†ã€‚ã€‚ã€‚')
    dataset = load_dataset('glue', dataset_name, cache_dir=data_dir,
                           download_config=resume_download_config)
    print(f"è®­ç»ƒé›†æ•°æ®å¤§å°: {len(dataset['train'])}")
    # print(f"æµ‹è¯•é›†æ•°æ®å¤§å°: {len(dataset['validation' if dataset_name != 'mnli' else 'validation_matched'])}")
    print('æ•°æ®é›†ä¸‹è½½å®Œæˆï¼')
    return dataset


def preprocess_dataset(dataset):
    # tokenizationæœŸé—´æœ‰è¶³å¤Ÿçš„ RAM æ¥å­˜å‚¨æ•´ä¸ªæ•°æ®é›†æ—¶è¿™ç§æ–¹æ³•æ‰æœ‰æ•ˆ
    # ï¼ˆè€Œ ğŸ¤— Datasets åº“ä¸­çš„æ•°æ®é›†æ˜¯å­˜å‚¨åœ¨ç£ç›˜ä¸Šçš„ Apache Arrow æ–‡ä»¶ï¼Œå› æ­¤æ‚¨åªéœ€å°†è¯·æ±‚åŠ è½½çš„æ ·æœ¬ä¿å­˜åœ¨å†…å­˜ä¸­ï¼‰ã€‚
    # æ‰€ä»¥æœ€å¥½æ˜¯ç”¨dataset.map
    if dataset_name in ['mrpc', 'rte', 'stsb']:
        # https://blog.csdn.net/qq_56591814/article/details/120147114
        # ä¸Šè¿°åšå®¢å†™çš„æ¯”è¾ƒè¯¦ç»†
        # ä¿ç•™lableåˆ—ï¼Œåç»­æµ‹è¯•è¦ç”¨åˆ°
        dataset = dataset.map(mrpc_rte_sstb_preprocess_function,
                              batched=True, remove_columns=["sentence1", "sentence2", "idx"])
    elif dataset_name in ['cola', 'sst2']:
        dataset = dataset.map(cola_sst2_preprocess_function,
                              batched=True, remove_columns=["sentence", "idx"])
    elif dataset_name in ['qqp']:
        dataset = dataset.map(qqp_preprocess_function,
                              batched=True, remove_columns=["question1", "question2", "idx"])
    elif dataset_name in ['qnli']:
        dataset = dataset.map(qnli_preprocess_function,
                              batched=True, remove_columns=["question", "sentence", "idx"])
    elif dataset_name in ['mnli']:
        dataset = dataset.map(mnli_preprocess_function,
                              batched=True, remove_columns=["premise", "hypothesis", "idx"])
    else:
        print("é”™è¯¯æ•°æ®é›†åç§°,æ•°æ®é¢„å¤„ç†å¤±è´¥ï¼Œé€€å‡ºï¼")
        sys.exit(0)

    print(f"\nç¼–ç åæ•°æ®é›†çš„ç‰¹å¾: {list(dataset['train'].features)}")
    # print('å¼€å§‹ä¿å­˜ç¼–ç åæ•°æ®é›†ã€‚ã€‚ã€‚')
    # dataset["train"].save_to_disk(f"{output_dir}/train".replace('/', os.sep))
    # dataset["validation"].save_to_disk(f"{output_dir}/eval".replace('/', os.sep))
    # print('ç¼–ç åæ•°æ®é›†ä¿å­˜å®Œæ¯•')
    return dataset


def mrpc_rte_sstb_preprocess_function(example):
    # ä¼šè‡ªåŠ¨æ·»åŠ èµ·å§‹ç»“æŸå’Œåˆ†éš”ç¬¦
    # ['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
    # [      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
    # tokenizationå‡½æ•°ä¸­çœç•¥äº†padding å‚æ•°ï¼Œè¿™æ˜¯å› ä¸ºpaddingåˆ°è¯¥æ‰¹æ¬¡ä¸­çš„æœ€å¤§é•¿åº¦æ—¶çš„æ•ˆç‡ï¼Œ
    # ä¼šé«˜äºæ‰€æœ‰åºåˆ—éƒ½paddingåˆ°æ•´ä¸ªæ•°æ®é›†çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚ å½“è¾“å…¥åºåˆ—é•¿åº¦å¾ˆä¸ä¸€è‡´æ—¶ï¼Œè¿™å¯ä»¥èŠ‚çœå¤§é‡æ—¶é—´å’Œå¤„ç†èƒ½åŠ›ï¼
    input = tokenizer(example["sentence1"], example["sentence2"], truncation=True)
    return input


def qqp_preprocess_function(example):
    input = tokenizer(example["question1"], example["question2"], truncation=True)
    return input


def qnli_preprocess_function(example):
    input = tokenizer(example["question"], example["sentence"], truncation=True)
    return input


def mnli_preprocess_function(example):
    input = tokenizer(example["premise"], example["hypothesis"], truncation=True)
    return input


def cola_sst2_preprocess_function(example):
    # ['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]']
    # [      0,      0,    0,     0,       0,          0,   0]
    # tokenizationå‡½æ•°ä¸­çœç•¥äº†padding å‚æ•°ï¼Œè¿™æ˜¯åŒæ ·çš„é“ç—›ã€‚
    input = tokenizer(example["sentence"], truncation=True)
    return input


def get_data_collator():
    if dataset_name in ['mrpc', 'cola', 'rte', 'sst2', 'qqp', 'qnli', 'stsb', 'mnli']:
        data_collator = DataCollatorWithPadding(
            tokenizer=tokenizer
        )
    else:
        print("é”™è¯¯æ•°æ®é›†åç§°,æ•°æ®æ”¶é›†å™¨ç”Ÿæˆå¤±è´¥ï¼Œé€€å‡ºï¼")
        sys.exit(0)
    return data_collator


def get_training_args():
    eval_save_steps = 10000 if dataset_name in ['mnli', 'qqp', 'qnli'] else 1000
    if dataset_name in ['mrpc', 'cola', 'rte', 'sst2', 'qqp', 'qnli', 'stsb', 'mnli']:
        training_args = TrainingArguments(
            output_dir=output_dir,
            # è®¾ç½®ä¸ºTrueæ—¶ï¼Œä¼šè‡ªåŠ¨å¯»æ‰¾é€‚åˆçš„æ‰¹æ¬¡å¤§å°ã€‚
            auto_find_batch_size=True,
            # åœ¨æ¯ä¸ªepochçš„æ­¥æ•°åè¿›è¡ŒéªŒè¯è¯„ä¼°
            evaluation_strategy="steps",
            # è®¾ç½®è¯„ä¼°çš„æ­¥æ•°é—´éš”
            eval_steps=eval_save_steps,
            # è®¾ç½®ä¿å­˜æ¨¡å‹çš„æ­¥æ•°é—´éš”
            save_steps=eval_save_steps,
            # è®¾ç½®ä¿å­˜çš„æ¨¡å‹æ•°é‡ä¸Šé™
            save_total_limit=10,
            num_train_epochs=epochs,
            per_device_train_batch_size=8,
            # æ¢¯åº¦ç´¯ç§¯ï¼Œå¯ä»¥å¸®åŠ©å¤„ç†æ˜¾å­˜ä¸è¶³çš„æƒ…å†µ
            gradient_accumulation_steps=1,
            learning_rate=learning_rate,
            weight_decay=0.01,
            adam_epsilon=1e-8,
            # å­¦ä¹ ç‡é¢„çƒ­çš„æ­¥æ•°
            warmup_steps=500,
            # è®­ç»ƒæ—¥å¿—å­˜å‚¨ç›®å½•
            logging_dir=f"{output_dir}/logs".replace('/', os.sep),
            # è®­ç»ƒæ—¥å¿—è¾“å‡ºçš„æ­¥æ•°é—´éš”
            logging_steps=1000,
            # æ˜¯å¦ç¦ç”¨tqdmè¿›åº¦æ¡
            disable_tqdm=False,
            # è®­ç»ƒç»“æŸåè‡ªåŠ¨åŠ è½½æœ€ä½³æ¨¡å‹
            load_best_model_at_end=True,
            seed=2023,
            report_to=["tensorboard"],
        )
    else:
        print("é”™è¯¯æ•°æ®é›†åç§°,è®­ç»ƒå‚æ•°è®¾ç½®å¤±è´¥ï¼Œé€€å‡ºï¼")
        sys.exit(0)
    return training_args


def get_trainer(training_args, tokenized_dataset, data_collator):
    trainer = Trainer(
        model,
        training_args,
        train_dataset=tokenized_dataset["train"],
        eval_dataset=tokenized_dataset["validation" if dataset_name != 'mnli' else 'validation_matched'],
        data_collator=data_collator,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )
    return trainer


def compute_metrics(eval_preds):
    """
    compute_metrics å‡½æ•°å¿…é¡»ä¼ å…¥ä¸€ä¸ª EvalPrediction å¯¹è±¡ä½œä¸ºå‚æ•°ã€‚ EvalPredictionæ˜¯ä¸€ä¸ªå…·æœ‰é¢„æµ‹å­—æ®µå’Œ label_ids å­—æ®µçš„å…ƒç»„ã€‚
    compute_metricsè¿”å›çš„ç»“æœæ˜¯å­—å…¸ï¼Œé”®å€¼å¯¹ç±»å‹åˆ†åˆ«æ˜¯stringså’Œfloatsï¼ˆstringsæ˜¯metricsçš„åç§°ï¼Œfloatsæ˜¯å…·ä½“çš„å€¼ï¼‰ã€‚
    ç›´æ¥è°ƒç”¨metricçš„computeæ–¹æ³•ï¼Œä¼ å…¥labelså’Œpredictionså³å¯å¾—åˆ°metricçš„å€¼ã€‚ä¹Ÿåªæœ‰è¿™æ ·åšæ‰èƒ½åœ¨è®­ç»ƒæ—¶å¾—åˆ°accã€F1ï¼ˆF1 = 2 * (ç²¾ç¡®åº¦ * å¬å›ç‡) / (ç²¾ç¡®åº¦ + å¬å›ç‡)ï¼‰
    ç­‰ç»“æœï¼ˆå…·ä½“æŒ‡æ ‡æ ¹æ®ä¸åŒä»»åŠ¡æ¥å®šï¼‰
    """
    if dataset_name in ['mrpc', 'cola', 'rte', 'sst2', 'qqp', 'qnli', 'stsb', 'mnli']:
        # æ ¹æ®æ•°æ®é›†ä¸åŒï¼Œä¸‹è½½ä¸åŒçš„metric
        metric = evaluate.load("glue", f'{dataset_name}', download_config=resume_download_config)
        # metric = load_metric("glue", f'{dataset_name}', download_config=resume_download_config)
        # æ‹¿å‡ºä¸€ä¸ªæµ‹è¯•å¯¹è±¡çš„eval_predsï¼Œå˜é‡åŒ…å«äº†æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„é¢„æµ‹ç»“æœå’Œå¯¹åº”çš„æ ‡ç­¾ã€‚
        # logitsè¡¨ç¤ºæ¨¡å‹å¯¹æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœï¼Œlabelsæ˜¯æ ·æœ¬å¯¹åº”çš„çœŸå®æ ‡ç­¾ã€‚
        # mrpcä¸­logitsæ˜¯ï¼ˆ408,2ï¼‰çš„æ•°ç»„ï¼Œç±»ä¼¼äºä¸‹é¢çš„æ•°ç»„ï¼Œæ¯ä¸€è¡Œçš„ä¸¤åˆ—ï¼Œåˆ†åˆ«è¡¨ç¤ºé¢„æµ‹ä¸ºç¬¬ä¸€ç±»çš„æ¦‚ç‡å’Œç¬¬äºŒç±»çš„æ¦‚ç‡ã€‚å³ç¬¬ä¸€è¡Œé¢„æµ‹ä¸¤ä¸ªå¥å­ç›¸å…³çš„æ¦‚ç‡ä¸º0.8
        # logits = [[-1.2, 0.8],
        #           [0.5, -0.3],
        #           [1.1, 2.4]]
        logits, labels = eval_preds
        # ä½¿ç”¨np.argmaxå‡½æ•°è·å–æ¯ä¸ªæ ·æœ¬çš„æœ€å¤§é¢„æµ‹å€¼çš„ç´¢å¼•ï¼Œå³é¢„æµ‹ç±»åˆ«ã€‚è¿™é‡Œå‡è®¾é¢„æµ‹ç»“æœæ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œé€šè¿‡å–æ¦‚ç‡æœ€é«˜çš„ç±»åˆ«ä½œä¸ºæœ€ç»ˆé¢„æµ‹ç»“æœã€‚
        # å¾—åˆ°çš„ç»“æ„ä¸º[1, 0, 1]ï¼Œå³æ¯ä¸€è¡Œå–æœ€å¤§å€¼æ‰€åœ¨çš„ä½ç½®ï¼Œç¬¬ä¸€è¡Œ0.8æ›´å¤§ï¼Œæ‰€ä»¥ä¸º1ï¼Œä»¥æ­¤ç±»æ¨
        # å¯¹äºstsbè¿™ç§å›å½’ä»»åŠ¡ï¼Œç®—ç›¸ä¼¼åº¦éœ€è¦å°†logits.squeeze(-1)ï¼Œå…¶ä¸­logits(n,1),lable(n,)ã€‚
        # ç„¶åç›´æ¥è®¡ç®—ç›¸ä¼¼åº¦ï¼Œè¿”å›{'pearson': float_num, 'spearmanr': float_num}
        predictions = logits.squeeze(-1) if dataset_name in ['stsb'] else np.argmax(logits, axis=-1)
        return metric.compute(predictions=predictions, references=labels)
    else:
        print("é”™è¯¯æ•°æ®é›†åç§°,è·å–metricå¤±è´¥ï¼Œé€€å‡ºï¼")
        sys.exit(0)


def train(tokenized_dataset):
    data_collator = get_data_collator()
    training_args = get_training_args()
    trainer = get_trainer(training_args, tokenized_dataset, data_collator)
    print("å¼€å§‹è®­ç»ƒ")
    trainer.train()

    print("è®­ç»ƒå®Œæ¯•ï¼Œä¿å­˜æ•°æ®")
    # save_path = f"{output_dir}/results".replace('/', os.sep)
    # trainer.model.save_pretrained(save_path)
    # tokenizer.save_pretrained(save_path)
    print("æ•°æ®ä¿å­˜å®Œæ¯•")


if __name__ == '__main__':
    try:
        # ç§‘å­¦ä¸Šç½‘é—®é¢˜ï¼Œé‡å¤è¿›è¡Œä¸‹è½½ï¼Œé¿å…ä¸‹è½½å¤±è´¥é€€å‡º
        resume_download_config = datasets.DownloadConfig(resume_download=True, max_retries=300)
        # è·å–model,tokenizer
        model, tokenizer = get_model_tokenizer()
        # è·å–æ•°æ®é›†
        dataset = get_dataset()
        # é¢„å¤„ç†æ•°æ®
        tokenized_dataset = preprocess_dataset(dataset)
        # å¼€å§‹è®­ç»ƒ
        train(tokenized_dataset)
        print('ç¨‹åºå®Œæˆï¼Œé€€å‡ºï¼')
        sys.exit(0)
    except SystemExit:
        sys.exit(0)
